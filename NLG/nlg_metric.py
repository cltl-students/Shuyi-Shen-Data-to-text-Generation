# coding=utf-8

from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
from nltk.translate import meteor
from nltk import word_tokenize
from statistics import mean
from nltk.corpus import wordnet
from nltk.translate.meteor_score import single_meteor_score
from bert_score import score



sm=SmoothingFunction().method1
#


def get_metric(pred_file, label_file):
    BLEU1=[]
    BLEU2=[]
    BLEU3=[]
    BLEU4=[]
    METEOR=[]
    preds = []
    labels = []
    with open(pred_file, 'r', encoding='utf8') as f:
        for line in f:
              preds.append(line)
    with open(label_file, 'r', encoding='utf8') as f:
        for line in f:
              labels.append(line)
    assert len(preds) == len(labels)
    for i in range(len(preds)):
        # target = 'the cat sat on the mat'  # target
        # inference = 'the cat is on the mat'  # inference
        target = labels[i]
        inference = preds[i]

        # tokenize
        target_fenci = ' '.join(word_tokenize(target))
        inference_fenci = ' '.join(word_tokenize(inference))

        # reference is the standard answer, it's a list. there could be different references, 
        #each of which is a sub list that was tokenized using split function 
        # # Example as follows
        # reference = [['this', 'is', 'a', 'duck']]
        reference = []  # gold sentence
        candidate = []  # sentences generated by nerual network
        # calculating Bleu
        reference.append(target_fenci.split())
        candidate = (inference_fenci.split())



        B1=sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=sm)
        BLEU1.append(B1)

        B2=sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=sm)
        BLEU2.append(B2)

        B3=sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=sm)
        BLEU3.append(B3)

        B4=sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=sm)
        BLEU4.append(B4)


        M = single_meteor_score(target, inference)
        METEOR.append(M)
        reference.clear()

    print('BLEU-1: %f' % mean(BLEU1))
    print('BLEU-2: %f' % mean(BLEU2))
    print('BLEU-3: %f' % mean(BLEU3))
    print('BLEU-4: %f' % mean(BLEU4))
    print('METEOR: %f' % round(mean(METEOR),4))



    P, R, F1 = score([pred_file], [label_file], lang="en", verbose=True)
    print(f"System level F1 score: {F1.mean():.3f}")

if __name__ == '__main__':
      pred_file = './result/t5_pred_final.txt'
        
      # pred_file = './result/t5_pred_10_percent.txt'
      # pred_file = './result/t5_pred_1_percent.txt'
  
      label_file = './result/t5_label_final.txt'
      get_metric(pred_file, label_file)
